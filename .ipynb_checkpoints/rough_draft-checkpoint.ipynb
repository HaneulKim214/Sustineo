{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3378734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7501ad08",
   "metadata": {},
   "source": [
    "Core Questions (3 total):\n",
    "\n",
    "1. \"Should employee business travel be classified as Scope 1 or Scope 3? Explain the\n",
    "reasoning and describe how I can calculate my business travel emissions?\"\n",
    "2. “Are my scope 2 emissions calculation valid according to the Greenhouse Gas\n",
    "Protocol?”\n",
    "3. How do my scope 1 & 2 emissions compare with other companies (refer to peer\n",
    "reports in the data sources) in my industry, and what insights can I derive from this\n",
    "comparison?\n",
    "\n",
    "Bonus Questions:\n",
    "\n",
    "1. \"What is our highest emitting Scope 3 category and what specific activities contribute\n",
    "to it?\"\n",
    "2. “Which suppliers should I prioritise to engage for emissions reduction efforts?”\n",
    "3. \"Generate a summary report of our total emissions by scope with key insights\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a0def5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scope1_df = pd.read_csv('data/raw/scope1.csv')\n",
    "scope2_df = pd.read_csv('data/raw/scope2.csv')\n",
    "scope3_df = pd.read_csv('data/raw/scope3.csv')\n",
    "\n",
    "\n",
    "scope1_df['scope'] = 'scope1'\n",
    "scope2_df['scope'] = 'scope2'\n",
    "scope3_df['scope'] = 'scope3'\n",
    "## Improvement: Some columns are referring to same data but in different names, should be combined.\n",
    " # ex: scope1_df['Activity_Type'] = scope2_df['Energy_Type'] = scope3_df['Activity_Description']\n",
    "scope_comb_df = pd.concat([scope1_df, scope2_df, scope3_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f51aba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scope1_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226e4610",
   "metadata": {},
   "source": [
    "## 1. Emission Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43eb37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "totals = {}\n",
    "totals['scope1'] = scope1_df['CO2e_Tonnes'].sum()\n",
    "totals['scope2'] = scope2_df['CO2e_Tonnes'].sum()\n",
    "totals['scope3'] = scope3_df['CO2e_Tonnes'].sum()\n",
    "totals['total'] = sum(totals.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f2d07f",
   "metadata": {},
   "source": [
    "### Get relevant information from vectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a76e0ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import fitz\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from typing import List, Dict\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e29c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# For Gemini, you need a Google API key (set as GOOGLE_API_KEY)\n",
    "import os\n",
    "chunk_size = 1000\n",
    "chunk_overlap = 200\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "rec_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f6879ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_name, path = 'ghg_protocol', 'data/raw/ghg-protocol-revised.pdf'\n",
    "\n",
    "doc = fitz.open(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bae73c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document('data/raw/ghg-protocol-revised.pdf')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5425cf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _clean_text(text: str) -> str:\n",
    "        \"\"\"Clean extracted text\"\"\"\n",
    "        # Remove excessive whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        # Remove special characters but keep punctuation\n",
    "        text = re.sub(r'[^\\w\\s\\-.,;:!?()]', '', text)\n",
    "        return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbf24a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "for page_num, page in enumerate(doc):\n",
    "    page_text = page.get_text()\n",
    "    # Clean and format text\n",
    "    page_text = _clean_text(page_text)\n",
    "    text += f\"\\n--- Page {page_num + 1} ---\\n{page_text}\\n\"\n",
    "\n",
    "# Split into chunks\n",
    "chunks = rec_text_splitter.split_text(text)\n",
    "\n",
    "# Add metadata\n",
    "documents = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    documents.append({\n",
    "        \"content\": chunk,\n",
    "        \"metadata\": {\n",
    "            \"source\": doc_name,\n",
    "            \"chunk_id\": i,\n",
    "            \"total_chunks\": len(chunks)\n",
    "        }\n",
    "    })\n",
    "\n",
    "texts = [doc[\"content\"] for doc in documents]\n",
    "metadatas = [doc[\"metadata\"] for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "144f0d88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': '--- Page 1 ---\\nA Corporate Accounting and Reporting Standard R E V I S E D E D I T I O N The Greenhouse Gas Protocol  390  370  350  330  310  290  270 ppm 1000 1500 2000 Year: W O R L D R E S O U R C E S I N S T I T U T E',\n",
       " 'metadata': {'source': 'ghg_protocol', 'chunk_id': 0, 'total_chunks': 586}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9178770e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': '--- Page 116 ---\\nW O R L D R E S O U R C E S I N S T I T U T E 10 G Street, NE (Suite 800) Washington, DC 20002 USA Tel: (1 202) 729 76 00 Fax: (1 202) 729 76 10 E-mail: sepinfo  wri.org Internet: www.wri.org 4, chemin de Conches 1231 Conches-Geneva Switzerland Tel: (41 22) 839 31 00 Fax: (41 22) 839 31 31 E-mail: info  wbcsd.org Internet: www.wbcsd.org',\n",
       " 'metadata': {'source': 'ghg_protocol', 'chunk_id': 585, 'total_chunks': 586}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343e477d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "FAISS.__init__() got an unexpected keyword argument 'index_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m vector_stores = {}\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m vector_store = \u001b[43mFAISS\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mFlat\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/project_green/lib/python3.13/site-packages/langchain_community/vectorstores/faiss.py:1044\u001b[39m, in \u001b[36mFAISS.from_texts\u001b[39m\u001b[34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[32m   1026\u001b[39m \n\u001b[32m   1027\u001b[39m \u001b[33;03mThis is a user friendly interface that:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1041\u001b[39m \u001b[33;03m        faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[32m   1042\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1043\u001b[39m embeddings = embedding.embed_documents(texts)\n\u001b[32m-> \u001b[39m\u001b[32m1044\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__from\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1045\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1046\u001b[39m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1047\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1048\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1049\u001b[39m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1050\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1051\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/project_green/lib/python3.13/site-packages/langchain_community/vectorstores/faiss.py:1004\u001b[39m, in \u001b[36mFAISS.__from\u001b[39m\u001b[34m(cls, texts, embeddings, embedding, metadatas, ids, normalize_L2, distance_strategy, **kwargs)\u001b[39m\n\u001b[32m   1002\u001b[39m docstore = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mdocstore\u001b[39m\u001b[33m\"\u001b[39m, InMemoryDocstore())\n\u001b[32m   1003\u001b[39m index_to_docstore_id = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mindex_to_docstore_id\u001b[39m\u001b[33m\"\u001b[39m, {})\n\u001b[32m-> \u001b[39m\u001b[32m1004\u001b[39m vecstore = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex_to_docstore_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnormalize_L2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnormalize_L2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdistance_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdistance_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1013\u001b[39m vecstore.__add(texts, embeddings, metadatas=metadatas, ids=ids)\n\u001b[32m   1014\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m vecstore\n",
      "\u001b[31mTypeError\u001b[39m: FAISS.__init__() got an unexpected keyword argument 'index_type'"
     ]
    }
   ],
   "source": [
    "vector_stores = {}\n",
    "vector_store = FAISS.from_texts(\n",
    "    texts=texts,\n",
    "    embedding=embeddings,\n",
    "    metadatas=metadatas,\n",
    "    index_type=\"Flat\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba966c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_stores[doc_name] = vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7c68f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Improvements:\n",
    "- Some irrelevant text is included in the text.\n",
    "- Ignore table of contents and unimportant texts in margins of pdf.\n",
    "\n",
    "ex:\n",
    "--- Page 3 ---\n",
    "2 6 10 16 24 34 40 48 58 62 68 74 86 88 90 92 95 96 103 104 Table of Contents G U I D A N C E S T A N D A R D G U I D A N C E S T A N D A R D G U I D A N C E S T A N D A R D G U I D A N C E S T A N D A R D G U I D A N C E S T A N D A R D G U I D A N C E G U I D A N C E G U I D A N C E G U I D A N C E G U I D A N C E G U I D A N C E G U I D A N C E G U I D A N C E S T A N D A R D Introduction The Greenhouse Gas Protocol Initiative Chapter 1 GHG Accounting and Reporting Principles Chapter 2 Business Goals and Inventory Design Chapter 3 Setting Organizational Boundaries Chapter 4 Setting Operational Boundaries Chapter 5 Tracking Emissions Over Time Chapter 6 Identifying and Calculating GHG Emissions Chapter 7 Managing Inventory Quality Chapter 8 Accounting for GHG Reductions Chapter 9 Reporting GHG Emissions Chapter 10 Verification of GHG Emissions Chapter 11 Setting GHG Targets Appendix A Accounting for Indirect Emissions from Electricity Appendix B Accounting for Sequestered Atmospheric Carbon Appendix C Overview of GHG Programs Appendix D Industry Sectors and Scopes Acronyms Glossary References Contributors\n",
    "\n",
    "\"\"\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263b70d3",
   "metadata": {},
   "source": [
    "### Summary from emissions data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19069bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scope_comb_df.groupby(['Category', 'scope'])[['CO2e_Tonnes']].sum()\n",
    "top_k = 3\n",
    "emitters_by_cat = scope3_df.groupby(['Category'])[['CO2e_Tonnes']].sum().reset_index()\n",
    "top3_emitters_by_cat = emitters_by_cat.sort_values(by='CO2e_Tonnes', ascending=False).iloc[:top_k]\n",
    "\n",
    "summary = f\"\"\"\n",
    "Total Emissions Overview:\n",
    "- Scope 1: {totals.get('scope1', 0):.2f} tCO2e\n",
    "- Scope 2: {totals.get('scope2', 0):.2f} tCO2e  \n",
    "- Scope 3: {totals.get('scope3', 0):.2f} tCO2e\n",
    "- Total: {totals.get('total', 0):.2f} tCO2e\n",
    "\n",
    "Data Records:\n",
    "- Scope 1: {len(scope1_df)} entries\n",
    "- Scope 2: {len(scope2_df)} entries\n",
    "- Scope 3: {len(scope3_df)} entries\n",
    "\"\"\"\n",
    "# Add top categories if available\n",
    "if not top3_emitters_by_cat.empty:\n",
    "    summary += \"\\n\\nTop3 Scope 3 Categories:\\n\"\n",
    "    ## Improvements: Avoid loop, print whole dataframe at once or pass markdown table.\n",
    "    for row in top3_emitters_by_cat.itertuples():\n",
    "        cat = getattr(row, 'Category')\n",
    "        emissions = getattr(row, 'CO2e_Tonnes')\n",
    "        summary += f\"- {cat}: {emissions:.2f} tCO2e\\n\"\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bd0dc2",
   "metadata": {},
   "source": [
    "## 2. Document Processor\n",
    "Read PDF then store in VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee44feb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9976c93",
   "metadata": {},
   "source": [
    "## 4. QA and actionable suggestion measurements\n",
    "\n",
    "How can Agent give suggestions:\n",
    "App1:\n",
    "1. We can use external (real-time) sources like News, regulations, etc... to retrieve information about changing trends/regulations\n",
    "2. feed this as context \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project green",
   "language": "python",
   "name": "project_green"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
